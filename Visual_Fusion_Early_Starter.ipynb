{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Visual Fusion Early - Starter.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "XGlg7uxki1Cq",
        "1rbhQKptg-uK"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itberrios/3D/blob/main/Visual_Fusion_Early_Starter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V2BxA_xar00"
      },
      "source": [
        "# Welcome to the Early Fusion Project\n",
        "\n",
        "Before we start, acknowledgement to this repo: https://github.com/kuixu/kitti_object_vis. This course has been based on this repo after seeing the great results and code! <p>\n",
        "\n",
        "We'll use the [KITTI Dataset](http://www.cvlibs.net/datasets/kitti/setup.php) to collect the Point Clouds, Images, and Calibration parameters. <p>\n",
        "\n",
        "After loading data from the dataset, our Early fusion process will happen in 3 steps:\n",
        "1.   **Project the Point Clouds (3D) to the Image(2D)** \n",
        "2.   **Detect Obstacles in 2D** (Camera)\n",
        "3.   **Fuse the Results**\n",
        "\n",
        "Are you ready? ‚úåüèº"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGlg7uxki1Cq"
      },
      "source": [
        "##0 - Load the Data and Visualize it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RYckDyji8H1"
      },
      "source": [
        "### Link Google Colab to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tk-izanQH9iY"
      },
      "source": [
        "# import os\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "# os.chdir(\"/content/drive/My Drive/Think Autonomous/SDC Course/Visual Fusion\")\n",
        "# !ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/itberrios/think_autonomous.git"
      ],
      "metadata": {
        "id": "Uvrjmd_qYRdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/think_autonomous/visual_fusion_course\"\n",
        "!ls"
      ],
      "metadata": {
        "id": "9iJjHWJ3YVaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrs7OD_Vi_Zs"
      },
      "source": [
        "### Import the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pppZPZ3_jBKN"
      },
      "source": [
        "!pip install open3d==0.12.0 # Version 12"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BC_e1h8MICKX"
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import glob\n",
        "import open3d as o3d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wr79FMSMpT8E"
      },
      "source": [
        "### Load the Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qP3-j5OAKhT2"
      },
      "source": [
        "image_files = sorted(glob.glob(\"data/img/*.png\"))\n",
        "point_files = sorted(glob.glob(\"data/velodyne/*.pcd\"))\n",
        "label_files = sorted(glob.glob(\"data/label/*.txt\"))\n",
        "calib_files = sorted(glob.glob(\"data/calib/*.txt\"))\n",
        "\n",
        "index = 0\n",
        "pcd_file = point_files[index]\n",
        "image = cv2.cvtColor(cv2.imread(image_files[index]), cv2.COLOR_BGR2RGB)\n",
        "cloud = o3d.io.read_point_cloud(pcd_file)\n",
        "points= np.asarray(cloud.points)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpMXag26GYwp"
      },
      "source": [
        "print(points[2,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rbhQKptg-uK"
      },
      "source": [
        "### Optional - If your LiDAR file is in binary extension '.bin', use this piece of code to turn it into a '.pcd' and save it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zq7lPkvYPXow"
      },
      "source": [
        "## BIN TO PCD\n",
        "import numpy as np\n",
        "import struct\n",
        "size_float = 4\n",
        "list_pcd = []\n",
        "\n",
        "file_to_open = point_files[index]\n",
        "file_to_save = str(point_files[index])[:-3]+\"pcd\"\n",
        "with open (file_to_open, \"rb\") as f:\n",
        "    byte = f.read(size_float*4)\n",
        "    while byte:\n",
        "        x,y,z,intensity = struct.unpack(\"ffff\", byte)\n",
        "        list_pcd.append([x, y, z])\n",
        "        byte = f.read(size_float*4)\n",
        "np_pcd = np.asarray(list_pcd)\n",
        "pcd = o3d.geometry.PointCloud()\n",
        "v3d = o3d.utility.Vector3dVector\n",
        "pcd.points = v3d(np_pcd)\n",
        "\n",
        "o3d.io.write_point_cloud(file_to_save, pcd)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00YaQZjrjRpW"
      },
      "source": [
        "### Visualize the Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnSsfviYjTxb"
      },
      "source": [
        "f, (ax1) = plt.subplots(1, 1, figsize=(20,10))\n",
        "ax1.imshow(image)\n",
        "ax1.set_title('Image', fontsize=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiNl7OdgjUri"
      },
      "source": [
        "### Visualize the Point Clouds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFRpDWNXjazV"
      },
      "source": [
        "!pip install pypotree #https://github.com/centreborelli/pypotree"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HqRVOwxJP8R"
      },
      "source": [
        "import pypotree \n",
        "cloudpath = pypotree.generate_cloud_for_display(points)\n",
        "pypotree.display_cloud_colab(cloudpath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44-hPZXmTulS"
      },
      "source": [
        "## 1 - Project the Points in the Image <p>\n",
        "That part is possibly the hardest to understand and will require your full attention. We want to project the 3D points into the image.<p>\n",
        "\n",
        "It means we'll need to: <p>\n",
        "\n",
        "*   Select the Point that are **visible** in the image ü§î\n",
        "*   Convert the Points **from the LiDAR frame to the Camera Frame** ü§Ø\n",
        "*   Find a way to project **from the Camera Frame to the Image Frame** üò≠\n",
        "\n",
        "<p>\n",
        "No worries here, we'll figure out everything together.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTqtmQJoBx-8"
      },
      "source": [
        "### 1.1 - Read the Calibration File\n",
        "\n",
        "The first step is to read the calibration files. For each image, we have an associated calibration file that states:<p>\n",
        "\n",
        "\n",
        "*   The instrinsic and extrinsic camera calibration parameters\n",
        "*   The velodyne to camera matrices\n",
        "*   All the other \"sensor A\" to \"sensor B\" matrices\n",
        "<p>\n",
        "They are made from this setup:<p>\n",
        "\n",
        "![link text](http://www.cvlibs.net/datasets/kitti/images/setup_top_view.png)\n",
        "\n",
        "Not everything matters to us here, only a few things:\n",
        "*    **Velo-To-Cam is a variable we'll call V2C** -- It gives the rotation and translation matrices from the Velodyne to the Left Grayscale camera.\n",
        "*    **R0_rect used in Stereo Vision to make the images co-planar.**\n",
        "*   **P2 is the matrix obtained after camera calibration**. It contains the intrinsic matrix K and the extrinsic.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSzz_3tf0xlX"
      },
      "source": [
        "class LiDAR2Camera(object):\n",
        "    def __init__(self, calib_file):\n",
        "        calibs = self.read_calib_file(calib_file)\n",
        "        P = calibs[\"P2\"]\n",
        "        self.P = np.reshape(P, [3, 4])\n",
        "        # Rigid transform from Velodyne coord to reference camera coord\n",
        "        V2C = calibs[\"Tr_velo_to_cam\"]\n",
        "        self.V2C = np.reshape(V2C, [3, 4])\n",
        "        # Rotation from reference camera coord to rect camera coord\n",
        "        R0 = calibs[\"R0_rect\"]\n",
        "        self.R0 = np.reshape(R0, [3, 3])\n",
        "\n",
        "    def read_calib_file(self, filepath):\n",
        "        \"\"\" Read in a calibration file and parse into a dictionary.\n",
        "        Ref: https://github.com/utiasSTARS/pykitti/blob/master/pykitti/utils.py\n",
        "        \"\"\"\n",
        "        data = {}\n",
        "        with open(filepath, \"r\") as f:\n",
        "            for line in f.readlines():\n",
        "                line = line.rstrip()\n",
        "                if len(line) == 0:\n",
        "                    continue\n",
        "                key, value = line.split(\":\", 1)\n",
        "                # The only non-float values in these files are dates, which\n",
        "                # we don't care about anyway\n",
        "                try:\n",
        "                    data[key] = np.array([float(x) for x in value.split()])\n",
        "                except ValueError:\n",
        "                    pass\n",
        "        return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZWuxKSFFEDA"
      },
      "source": [
        "lidar2cam = LiDAR2Camera(calib_files[index])\n",
        "print(\"P :\"+str(lidar2cam.P))\n",
        "print(\"-\")\n",
        "print(\"RO \"+str(lidar2cam.R0))\n",
        "print(\"-\")\n",
        "print(\"Velo 2 Cam \" +str(lidar2cam.V2C))\n",
        "print(\"-\")\n",
        "#print(\"Cam 2 Velo\" + str(lidar2cam.C2V))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZ4qEcG9CNJq"
      },
      "source": [
        "### 1.2 - Project the Points in the Image\n",
        "\n",
        "The main formula we'll use will be as follows:<p>\n",
        "**Y(2D) = P x R0 x R|t x X (3D)** \n",
        "\n",
        "However, when looking at the dimensions:\n",
        "\n",
        "*   P: [3x4]\n",
        "*   R0: [3x3]\n",
        "*   R|t = Velo2Cam: [3x4]\n",
        "*   X: [3x1]\n",
        "\n",
        "We'll need to convert some points into Homogeneous Coordinates:\n",
        "* RO must go from 3x3 to 4x3\n",
        "* x must go from 3x1 to 4x1\n",
        "\n",
        "Then, to retrieve the cartesian system, we'll divide as explained in the course."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jB2aXeITxS8"
      },
      "source": [
        "def cart2hom(self, pts_3d):\n",
        "    \"\"\" Input: nx3 points in Cartesian\n",
        "        Oupput: nx4 points in Homogeneous by pending 1\n",
        "    \"\"\"\n",
        "    n = pts_3d.shape[0]\n",
        "    pts_3d_hom = np.hstack((pts_3d, np.ones((n, 1))))\n",
        "    return pts_3d_hom\n",
        "\n",
        "LiDAR2Camera.cart2hom = cart2hom"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PUKxLc5HFsd"
      },
      "source": [
        "def project_velo_to_ref(self, pts_3d_velo):\n",
        "    pts_3d_velo = self.cart2hom(pts_3d_velo)  # nx4\n",
        "    return np.dot(pts_3d_velo, np.transpose(self.V2C))\n",
        "\n",
        "LiDAR2Camera.project_velo_to_ref = project_velo_to_ref\n",
        "\n",
        "print(points[:1,:])\n",
        "print(lidar2cam.project_velo_to_ref(points[:1,:]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxhDF3puLpXq"
      },
      "source": [
        "--- TODO ---\n",
        "Code the Project_velo_to_image function and test it for the first 5 points. Make sure it makes sense."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siG14wbNcVCK"
      },
      "source": [
        "def project_velo_to_image(self, pts_3d_velo):\n",
        "    '''\n",
        "    Input: 3D points in Velodyne Frame [nx3]\n",
        "    Output: 2D Pixels in Image Frame [nx2]\n",
        "    '''\n",
        "    # REVERSE TECHNIQUE\n",
        "    '''\n",
        "    homogeneous = self.cart2hom(pts_3d_velo)  # nx4\n",
        "    dotted_RT = np.dot(homogeneous, np.transpose(self.V2C)) #nx3\n",
        "    dotted_with_RO = np.transpose(np.dot(self.R0, np.transpose(dotted_RT))) #nx3\n",
        "    homogeneous_2 = self.cart2hom(dotted_with_RO) #nx4\n",
        "    pts_2d = np.dot(homogeneous_2, np.transpose(self.P))  # nx3\n",
        "    '''\n",
        "    \n",
        "    # NORMAL TECHNIQUE\n",
        "    R0_homo = np.vstack([self.R0, [0, 0, 0]])\n",
        "    R0_homo_2 = np.column_stack([R0_homo, [0, 0, 0, 1]])\n",
        "    p_r0 = np.dot(self.P, R0_homo_2) #PxR0\n",
        "    p_r0_rt =  np.dot(p_r0, np.vstack((self.V2C, [0, 0, 0, 1]))) #PxROxRT\n",
        "    pts_3d_homo = np.column_stack([pts_3d_velo, np.ones((pts_3d_velo.shape[0],1))])\n",
        "    p_r0_rt_x = np.dot(p_r0_rt, np.transpose(pts_3d_homo))#PxROxRTxX\n",
        "    pts_2d = np.transpose(p_r0_rt_x)\n",
        "    \n",
        "    pts_2d[:, 0] /= pts_2d[:, 2]\n",
        "    pts_2d[:, 1] /= pts_2d[:, 2]\n",
        "    return pts_2d[:, 0:2]\n",
        "\n",
        "\n",
        "LiDAR2Camera.project_velo_to_image = project_velo_to_image\n",
        "print(points[:5,:3])\n",
        "print(\"Euclidean Pixels \"+str(lidar2cam.project_velo_to_image(points[:5,:3])))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0NSdCLpCgBP"
      },
      "source": [
        "### 1.4 - LiDAR in Image Field Of View"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cICtSHfD8I8Z"
      },
      "source": [
        "def get_lidar_in_image_fov(self,pc_velo, xmin, ymin, xmax, ymax, return_more=False, clip_distance=2.0):\n",
        "    \"\"\" Filter lidar points, keep those in image FOV \"\"\"\n",
        "    pts_2d = self.project_velo_to_image(pc_velo)\n",
        "    fov_inds = (\n",
        "        (pts_2d[:, 0] < xmax)\n",
        "        & (pts_2d[:, 0] >= xmin)\n",
        "        & (pts_2d[:, 1] < ymax)\n",
        "        & (pts_2d[:, 1] >= ymin)\n",
        "    )\n",
        "    fov_inds = fov_inds & (pc_velo[:, 0] > clip_distance) # We don't want things that are closer to the clip distance (2m)\n",
        "    imgfov_pc_velo = pc_velo[fov_inds, :]\n",
        "    if return_more:\n",
        "        return imgfov_pc_velo, pts_2d, fov_inds\n",
        "    else:\n",
        "        return imgfov_pc_velo\n",
        "    \n",
        "LiDAR2Camera.get_lidar_in_image_fov = get_lidar_in_image_fov"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVNFq-3gCnmL"
      },
      "source": [
        "###1.5 -- Get the LiDAR Points in Pixels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DOf4AXe8Kqf"
      },
      "source": [
        "def show_lidar_on_image(self, pc_velo, img, debug=\"False\"):\n",
        "    \"\"\" Project LiDAR points to image \"\"\"\n",
        "    imgfov_pc_velo, pts_2d, fov_inds = self.get_lidar_in_image_fov(\n",
        "        pc_velo, 0, 0, img.shape[1], img.shape[0], True\n",
        "    )\n",
        "    if (debug==True):\n",
        "        print(\"3D PC Velo \"+ str(imgfov_pc_velo)) # The 3D point Cloud Coordinates\n",
        "        print(\"2D PIXEL: \" + str(pts_2d)) # The 2D Pixels\n",
        "        print(\"FOV : \"+str(fov_inds)) # Whether the Pixel is in the image or not\n",
        "    self.imgfov_pts_2d = pts_2d[fov_inds, :]\n",
        "    '''\n",
        "    #homogeneous = np.hstack((imgfov_pc_velo, np.ones((imgfov_pc_velo.shape[0], 1))))\n",
        "    homogeneous = self.cart2hom(imgfov_pc_velo)\n",
        "    transposed_RT = np.dot(homogeneous, np.transpose(self.V2C))\n",
        "    dotted_RO = np.transpose(np.dot(self.R0, np.transpose(transposed_RT)))\n",
        "    self.imgfov_pc_rect = dotted_RO\n",
        "    \n",
        "    if debug==True:\n",
        "        print(\"FOV PC Rect \"+ str(self.imgfov_pc_rect))\n",
        "    '''\n",
        "    cmap = plt.cm.get_cmap(\"hsv\", 256)\n",
        "    cmap = np.array([cmap(i) for i in range(256)])[:, :3] * 255\n",
        "    self.imgfov_pc_velo = imgfov_pc_velo\n",
        "    \n",
        "    for i in range(self.imgfov_pts_2d.shape[0]):\n",
        "        #depth = self.imgfov_pc_rect[i,2]\n",
        "        #print(depth)\n",
        "        depth = imgfov_pc_velo[i,0]\n",
        "        #print(depth)\n",
        "        color = cmap[int(510.0 / depth), :]\n",
        "        cv2.circle(\n",
        "            img,(int(np.round(self.imgfov_pts_2d[i, 0])), int(np.round(self.imgfov_pts_2d[i, 1]))),2,\n",
        "            color=tuple(color),\n",
        "            thickness=-1,\n",
        "        )\n",
        "\n",
        "    return img, self.imgfov_pts_2d\n",
        "\n",
        "LiDAR2Camera.show_lidar_on_image = show_lidar_on_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaLfy002oOIG"
      },
      "source": [
        "#img_3 = lidar2cam.show_lidar_on_image(points[:,:3], image)\n",
        "img_3 = image.copy()\n",
        "img_3, imgfov_pts_2d = lidar2cam.show_lidar_on_image(points[:,:3], img_3)\n",
        "plt.figure(figsize=(14,7))\n",
        "plt.imshow(img_3)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmYX4Iex6JzH"
      },
      "source": [
        "## 2 - Detect Obstacles in 2D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTEnwwS47BcJ"
      },
      "source": [
        "!python3 -m pip install yolov4==2.0.2 # After Checking, YOLO 2.0.2 works without modifying anything. Otherwise keep 1.2.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-Py5K64gDvQ"
      },
      "source": [
        "from yolov4.tf import YOLOv4\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "yolo = YOLOv4(tiny=False)\n",
        "yolo.classes = \"data/YOLOv4/coco.names\"\n",
        "yolo.make_model()\n",
        "yolo.load_weights(\"data/Yolov4/yolov4-tiny.weights\", weights_type=\"yolo-tiny\")\n",
        "\n",
        "def run_obstacle_detection(img):\n",
        "    start_time=time.time()\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    resized_image = yolo.resize_image(img)\n",
        "    # 0 ~ 255 to 0.0 ~ 1.0\n",
        "    resized_image = resized_image / 255.\n",
        "    #input_data == Dim(1, input_size, input_size, channels)\n",
        "    input_data = resized_image[np.newaxis, ...].astype(np.float32)\n",
        "\n",
        "    candidates = yolo.model.predict(input_data)\n",
        "\n",
        "    _candidates = []\n",
        "    result = img.copy()\n",
        "    for candidate in candidates:\n",
        "        batch_size = candidate.shape[0]\n",
        "        grid_size = candidate.shape[1]\n",
        "        _candidates.append(tf.reshape(candidate, shape=(1, grid_size * grid_size * 3, -1)))\n",
        "        #candidates == Dim(batch, candidates, (bbox))\n",
        "        candidates = np.concatenate(_candidates, axis=1)\n",
        "        #pred_bboxes == Dim(candidates, (x, y, w, h, class_id, prob))\n",
        "        pred_bboxes = yolo.candidates_to_pred_bboxes(candidates[0], iou_threshold=0.35, score_threshold=0.40)\n",
        "        pred_bboxes = pred_bboxes[~(pred_bboxes==0).all(1)] #https://stackoverflow.com/questions/35673095/python-how-to-eliminate-all-the-zero-rows-from-a-matrix-in-numpy?lq=1\n",
        "        pred_bboxes = yolo.fit_pred_bboxes_to_original(pred_bboxes, img.shape)\n",
        "        exec_time = time.time() - start_time\n",
        "        #print(\"time: {:.2f} ms\".format(exec_time * 1000))\n",
        "        result = yolo.draw_bboxes(img, pred_bboxes)\n",
        "        result = cv2.cvtColor(result, cv2.COLOR_BGR2RGB)\n",
        "    return result, pred_bboxes\n",
        "\n",
        "result, pred_bboxes = run_obstacle_detection(image)\n",
        "\n",
        "fig_camera = plt.figure(figsize=(14, 7))\n",
        "ax_lidar = fig_camera.subplots()\n",
        "ax_lidar.imshow(result)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Use yolov5 instead"
      ],
      "metadata": {
        "id": "8K8ORZPtcvMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ultralytics/yolov5"
      ],
      "metadata": {
        "id": "5MqUEgdlcOeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r yolov5/requirements.txt  #Install whatever is needed"
      ],
      "metadata": {
        "id": "47ALfxvRag2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # or yolov5m, yolov5l, yolov5x, custom"
      ],
      "metadata": {
        "id": "4G-xinP8c9QZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detections = model(image)"
      ],
      "metadata": {
        "id": "gBs05Cv5dBVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detections.show()"
      ],
      "metadata": {
        "id": "XMSPPzMydIPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get bounding box (xy, xy) locations, confidence, and MSCOCO category\n",
        "detections.xyxy"
      ],
      "metadata": {
        "id": "z4F60EB4dKUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJFi74VA7F0F"
      },
      "source": [
        "## 3 - Fuse Points Clouds & Bounding Boxes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(detections.render()[0])"
      ],
      "metadata": {
        "id": "vI4nY0mbeAE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbox_img = detections.render()[0].copy()\n",
        "new_img, _ = lidar2cam.show_lidar_on_image(points[:,:3], bbox_img)\n",
        "plt.figure(figsize=(14,7))\n",
        "plt.imshow(new_img)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "d83TIawceLvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3q6V1HE265jk"
      },
      "source": [
        "lidar_img_with_bboxes= yolo.draw_bboxes(img_3, pred_bboxes)\n",
        "fig_fusion = plt.figure(figsize=(14, 7))\n",
        "ax_fusion = fig_fusion.subplots()\n",
        "ax_fusion.imshow(lidar_img_with_bboxes)\n",
        "plt.show()\n",
        "cv2.imwrite(\"output/lidar_bboxes.png\", lidar_img_with_bboxes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0Qebe-FHViZ"
      },
      "source": [
        "**In this course, we'll see a few ways to filter outliers.** <p>\n",
        "Outliers are the points that belong to the bounding box, but not to the object.<p>\n",
        "Here's an example of outliers:<p>\n",
        "![outlier image](https://i.ibb.co/Fg0KV3k/Screenshot-2021-05-31-at-22-31-29.png)\n",
        "\n",
        "In this image, the points belong to the truck, but are also counted as part of the car.\n",
        "\n",
        "The first technique we can use for that is a shrink factor.\n",
        "Instead of considering the whole bounding box, we're considering only a part of it. **A common choice is 10-15% shrinking.**\n",
        "![image_shrinks](https://i.ibb.co/Zcgzz6F/Screenshot-2021-05-31-at-22-45-36.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pihcxsm7MWrk"
      },
      "source": [
        "--- TODO --- Code a function that return the points inside a bounding box according to a shrink factor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9VW-osNGbpe"
      },
      "source": [
        "def rectContains(rect, pt, w, h, shrink_factor=0):      \n",
        "     \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DewS97_cLjxJ"
      },
      "source": [
        "**The second way will be through Outlier removal techniques. <p>**\n",
        "We can cite a few: 3 Sigma, RANSAC, and others..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOJamotbMxmR"
      },
      "source": [
        "--- TODO--- Code a function to remove the outliers according to One Sigma"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bk6HfEgAsg9R"
      },
      "source": [
        "import statistics\n",
        "import random\n",
        "\n",
        "def filter_outliers(distances):\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qfu41CIoM4oY"
      },
      "source": [
        "--- TODO --- Code a function to get the Best Distance according to at least 3 criterias of your choice (closest, average, median, farthest, ...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgCLQn9HM39S"
      },
      "source": [
        "def get_best_distance(distances, technique=\"closest\"):\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTQBWN-vNQzL"
      },
      "source": [
        "-- TODO-- Code a function that implements the fusion between boxes and points"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.round(imgfov_pts_2d[:, 0]).astype(int)\n",
        "y = np.round(imgfov_pts_2d[:, 1]).astype(int)"
      ],
      "metadata": {
        "id": "pvg_6lNEg8_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "id": "Gc6ydTiGhs3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_bboxes = detections.xyxy[0][:, :4].numpy().round().astype(int)\n",
        "pred_bboxes[0, :]"
      ],
      "metadata": {
        "id": "A3ORUiHJgbc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(x < 794)"
      ],
      "metadata": {
        "id": "injiviPhhty2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbox = pred_bboxes[0]\n",
        "\n",
        "idxs = (x > bbox[0] * (1 - 0.25)) \\\n",
        "             & (y > bbox[1] * (1 - 0.25)) \\\n",
        "             & (x < bbox[2] * (1 - 0.25)) \\\n",
        "             & (y < bbox[3] * (1 - 0.25))\n",
        "\n",
        "idxs = (x > bbox[0] * (1 + 0.05)) \\\n",
        "             & (y > bbox[1]) \\\n",
        "             & (x < bbox[2] * (1 - 0.05)) \\\n",
        "             & (y < bbox[3])\n",
        "\n",
        "idxs = (x > bbox[0] * (1 + 0.05)) \\\n",
        "             & (y > bbox[1] * (1 + 0.05)) \\\n",
        "             & (x < bbox[2] * (1 - 0.05)) \\\n",
        "             & (y < bbox[3] * (1 - 0.05))\n",
        "\n",
        "# idxs = (x > bbox[0]) \\\n",
        "#              & (y > bbox[1]) \\\n",
        "#              & (x < bbox[2]) \\\n",
        "#              & (y < bbox[3])\n",
        "\n",
        "filtered_points = imgfov_pts_2d[idxs, :]"
      ],
      "metadata": {
        "id": "08G32YnIiQDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_points"
      ],
      "metadata": {
        "id": "4lqrVWSqkWlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp = detections.render()[0].copy()"
      ],
      "metadata": {
        "id": "MfLPWiZ9istQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(tmp)"
      ],
      "metadata": {
        "id": "IBmCjHgiiuEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for pt in filtered_points:\n",
        "    cv2.circle(tmp, np.round(pt).astype(int), 1, (0, 255, 0), -1)"
      ],
      "metadata": {
        "id": "BzQL3QfKikUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(14, 7))\n",
        "plt.imshow(tmp)"
      ],
      "metadata": {
        "id": "M2GUt33ljc0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JF8wnxMuj_zQ"
      },
      "source": [
        "def lidar_camera_fusion(self, pred_bboxes, image, shrink_factor=0.0125):\n",
        "\n",
        "    img = image.copy()\n",
        "\n",
        "    cmap = plt.cm.get_cmap(\"hsv\", 256)\n",
        "    cmap = np.array([cmap(i) for i in range(256)])[:, :3] * 255\n",
        "    distances = []\n",
        "\n",
        "\n",
        "    # projected LiDAR points\n",
        "    x = np.round(self.imgfov_pts_2d[:, 0]).astype(int)\n",
        "    y = np.round(self.imgfov_pts_2d[:, 1]).astype(int)\n",
        "\n",
        "    for bbox in pred_bboxes:\n",
        "      \n",
        "        # filter all points outside of each bbox with added shrink factor\n",
        "        idxs = (x > bbox[0] * (1 + shrink_factor)) \\\n",
        "              & (y > bbox[1] * (1 + shrink_factor)) \\\n",
        "              & (x < bbox[2] * (1 - shrink_factor)) \\\n",
        "              & (y < bbox[3] * (1 - shrink_factor))\n",
        "\n",
        "        if idxs.sum() > 0:\n",
        "\n",
        "            filtered_points = self.imgfov_pts_2d[idxs, :]\n",
        "            filtered_distances = self.imgfov_pc_velo[idxs, 0]\n",
        "          \n",
        "            _distances = []\n",
        "\n",
        "            \n",
        "            for pt, depth in zip(filtered_points, filtered_distances):\n",
        "                _distances.append(depth)\n",
        "                color = cmap[int(510.0 / depth), :]\n",
        "\n",
        "                cv2.circle(img, np.round(pt).astype(int), 1, color, -1)\n",
        "\n",
        "            # get median distance\n",
        "            distances.append(np.median(_distances))\n",
        "\n",
        "            # draw distance on the image\n",
        "\n",
        "            # get bbox dimensions\n",
        "            w = bbox[2] - bbox[0]\n",
        "            h = bbox[3] - bbox[1]\n",
        "\n",
        "            cv2.putText(img, '{0:.2f} m'.format(distances[-1]), \n",
        "                        (int(bbox[0] + w//2), int(bbox[1] + h//2)), \n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.75, (255, 0, 0), 3, cv2.LINE_AA)  \n",
        "            \n",
        "        else:\n",
        "          distances.append(np.nan) # don't worry about relatively small detections\n",
        "\n",
        "    return img, distances\n",
        "\n",
        "LiDAR2Camera.lidar_camera_fusion = lidar_camera_fusion"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6w7bFcYk03p"
      },
      "source": [
        "result = detections.render()[0].copy()\n",
        "final_result, _ = lidar2cam.lidar_camera_fusion(pred_bboxes, result)\n",
        "\n",
        "fig_keeping = plt.figure(figsize=(14, 7))\n",
        "ax_keeping = fig_keeping.subplots()\n",
        "ax_keeping.imshow(final_result)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlrlH991R34T"
      },
      "source": [
        "### Build a Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWhXl-ftiIA6"
      },
      "source": [
        "def pipeline (self, image, point_cloud):\n",
        "    \"For a pair of 2 Calibrated Images\"\n",
        "    img = image.copy()\n",
        "    # Show LidAR on Image\n",
        "    lidar_img = self.show_lidar_on_image(point_cloud[:,:3], image)\n",
        "    # Run obstacle detection in 2D\n",
        "    # result, pred_bboxes = run_obstacle_detection(img)\n",
        "    detections = model(img)\n",
        "    pred_bboxes = detections.xyxy[0][:, :4].numpy().round().astype(int) # get bbox coordinates\n",
        "    result = detections.render()[0]                             # get image with drawn bboxes\n",
        "    # Fuse Point Clouds & Bounding Boxes\n",
        "    img_final, _ = self.lidar_camera_fusion(pred_bboxes, result)\n",
        "    return img_final\n",
        "\n",
        "LiDAR2Camera.pipeline = pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhtJIYtTGLX1"
      },
      "source": [
        "image_files = sorted(glob.glob(\"data/img/*.png\"))\n",
        "point_files = sorted(glob.glob(\"data/velodyne/*.pcd\"))\n",
        "label_files = sorted(glob.glob(\"data/label/*.txt\"))\n",
        "calib_files = sorted(glob.glob(\"data/calib/*.txt\"))\n",
        "\n",
        "lidar2cam = LiDAR2Camera(calib_files[index])\n",
        "cloud = o3d.io.read_point_cloud(pcd_file)\n",
        "points= np.asarray(cloud.points)\n",
        "\n",
        "index = 0\n",
        "image = cv2.cvtColor(cv2.imread(image_files[index]), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "plt.figure(figsize=(14,7))\n",
        "final_result = lidar2cam.pipeline(image.copy(), points)\n",
        "plt.imshow(final_result)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAnmN-M6z5o_"
      },
      "source": [
        "## Comparing with the Ground Truth\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r07ai06Pz7TT"
      },
      "source": [
        "image_gt = image.copy()\n",
        "\n",
        "with open(label_files[index], 'r') as f:\n",
        "    fin = f.readlines()\n",
        "    for line in fin:\n",
        "        if line.split(\" \")[0] != \"DontCare\":\n",
        "            #print(line)\n",
        "            x1_value = int(float(line.split(\" \")[4]))\n",
        "            y1_value = int(float(line.split(\" \")[5]))\n",
        "            x2_value = int(float(line.split(\" \")[6]))\n",
        "            y2_value = int(float(line.split(\" \")[7]))\n",
        "            dist = float(line.split(\" \")[13])\n",
        "            cv2.rectangle(image_gt, (x1_value, y1_value), (x2_value, y2_value), (0,205,0), 10)\n",
        "            cv2.putText(image_gt, str(dist), (int((x1_value+x2_value)/2),int((y1_value+y2_value)/2)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2, cv2.LINE_AA)    \n",
        "\n",
        "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(30,20))\n",
        "ax1.imshow(image_gt)\n",
        "ax1.set_title('Ground Truth', fontsize=30)\n",
        "ax2.imshow(final_result) # or flag\n",
        "ax2.set_title('Prediction', fontsize=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eZq-Put27fl"
      },
      "source": [
        "## Shooting a Portfolio Video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VieqzsWS2-Wk"
      },
      "source": [
        "video_images = sorted(glob.glob(\"data/video/images/*.png\"))\n",
        "video_points = sorted(glob.glob(\"data/video/points/*.pcd\"))\n",
        "\n",
        "# Build a LiDAR2Cam object\n",
        "lidar2cam_video = LiDAR2Camera(calib_files[0])\n",
        "\n",
        "result_video = []\n",
        "\n",
        "for idx, img in enumerate(video_images):\n",
        "    image = cv2.cvtColor(cv2.imread(img), cv2.COLOR_BGR2RGB)\n",
        "    point_cloud = np.asarray(o3d.io.read_point_cloud(video_points[idx]).points)\n",
        "    result_video.append(lidar2cam_video.pipeline(image, point_cloud))\n",
        "\n",
        "out = cv2.VideoWriter('out.avi',cv2.VideoWriter_fourcc(*'DIVX'), 15, (image.shape[1],image.shape[0]))\n",
        " \n",
        "for i in range(len(result_video)):\n",
        "    out.write(cv2.cvtColor(result_video[i], cv2.COLOR_BGR2RGB))\n",
        "    #out.write(result_video[i])\n",
        "out.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpE17S2vjP8w"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "VZJAh69-2bOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BNPYWDpE2eat"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}